###
#! Design and Features
###
###
# Design overview
###
# DSC is designed to execute a sequence of computational steps that generate (or gather) data, perform statistical analyses and evaluate the performance of statistical procedures involved. Each step will take some input and create some output. A DSC user thus has two jobs: define the steps, and define the sequence of steps.
# A typical DSC sequences consists of 3 logical types of steps: ""scenario"", ""method"" and ""score"", where
#{list
# ""scenario"": provides input data and / or computational routines that generate input data.
# ""method"": defines statistical procedures that analyzes data.
# ""score"": defines methods that evaluates the result of data analyses against the ``truth'' (model from which the data is generated) and calculates scores as measures of evaluations.
#}
# DSC allows for flexible combination of computational steps, as illustrated in the pipeline below,
#{out
| scenario 1               |    | method 1 ->                      |    | score 1 |
| scenario 2               | -> | method 2 -> method 1             | -> | score 2 |
| scenario 1 -> scenario 3 |    | method 3 -> method 2 -> method 1 |    | ...     |
| scenario 4 -> ...        |    | method 4 -> ...                  |    | ...     |
#}
# where each ""scenario"", ""method"" and ""score"" is a computational step which can be different approaches to generate data, perform statistical analysis, or measure performance of methods, or can be the same approaches with different parameter settings. It is good to have the 3 logical types in mind when developing a DSC, though these are not necessarily keywords in DSC syntax, as will be demonstrated in the case study below.
###
# A case study
###
# To understand the DSC design we review a simple example with (mostly) self-explanatory syntax. DSC syntax is completely documented [elsewhere|@DSC2-Syntax@]; readers should not worry about syntax at this point.
# The example reads:
#{out yaml
output:///cat ../vignettes/one_sample_location_qnorm/settings.yaml
#}
# Each section in the DSC file, except for the last ``DSC'' section, is called a "DSC block". Each DSC block defines a family of computational routines (the @@exec@@), input to these routines (@@params@@) and output (@@return@@). The family of routines in the same block share similar (not necessarily the same) input parameters, and strictly the same return values. In this example, computational steps from the same block are logically concurrent to each other, because routines for all @@exec@@ entries are independent. The ``DSC'' section defines the sequence of steps to execute, via logical combinations of available DSC blocks.
# In this example, the first part of the DSC sequence calls the ``simulate'' block, which has two computational routines @@rcauchy.R@@ and @@rt.R@@. These routines generate @@n@@ random samples under Cauchy distribution with location parameter @@true_loc@@, and "t" distribution with non-centrality parameter @@true_loc@@, respectively. There are 2 choices of computational routines, each routine has one choice of @@n@@ (1000) and two choices of @@true_loc@@ (0 and 1). 5 replicates are involved as defined by @@seed@@. As a result, the ``simulate'' block has 20 parallel computational steps.
# The second part of the sequence consists of ``transform'' and ``estimate'' blocks. ``transform'' block takes input data and performs quantile normalization. ``estimate'' block defines two computational routines to estimate location parameter, via sample mean (@@mean.R@@) and median (@@median.R@@) respectively. In this part, there are two types of procedures: the first is @@transform + estimate@@ which runs the ``transform'' family of steps first, then run the ``estimate'' family after data has been transformed; the second is @@estimate@@ which directly performs parameter estimation with the original data produced by its upstream computational steps.
# The last part of the sequence calls the ``mse'' block, which has a single computational routine @@MSE.R@@ to calculate the mean square error as a summary of comparison between the "true" (@@true_mean@@) and "estimated" (@@mean_est@@) location parameters, taking @@$true_loc@@ and @@$loc@@ respectively which correspond to the @@return@@ values from previous computational steps.
# In this case study, the design of blocks logically follows the ""scenario"", ""method"" (pre-processing and analysis), and ""score"" paradigm. It does not have to be this way, though: one can even make separated blocks for each computational routine (in this case for every R script) and combine them in the "run" sequence in "DSC" section. In this case it appears to be the most reasonable choice because computational routines sharing input and output are consolidated into single blocks and logic between blocks are clearly presented.
# A crucial point in DSC design is the scope of variables starting with @@$@@ symbol. Within a DSC block, all parameter variables should be considered "local" to the block, not accessible from outside the block; all returned values should be considered "global" variable accessible from other DSC blocks via @@$@@. It is legitimate and sometimes necessary that multiple blocks have the same variable name in their return entry, for example here both ``simulate'' and ``transform'' blocks return @@x@@. In such case, which block provides value @@$x@@ depends on the context of the DSC sequence. DSC will always search for the nearest upstream block that returns the value needed. For example, the DSC sequence @@simulate * (transform * estimate, estimate) * mse@@ can be expanded to @@simulate * transform * estimate * mse@@ and @@simulate * estimate * mse@@. In the first sequence, @@$x@@ in ``estimate'' comes from its direct upstream neighbor ``transform'', not ``simulate'', while in the second sequence @@$x@@ comes from ``simulate''.
